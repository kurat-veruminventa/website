---
template: SinglePost
title: Combining Context and Input Methods
status: Published
date: '2019-10-31'
author: 'Haakon Fuhre Pettersen'
authorlink: /team/haakon/
role: 'Chief Technology Officer'
portrait: 'https://cdn.image4.io/ntention/f_auto/Team/050ff233-c8ca-44ec-8c0a-b4de723884a6.Jpeg'
email: mailto:hfp@ntention.com
linkedin: https://www.linkedin.com/in/haakon-fuhre-pettersen/
featuredImage: 'https://cdn.image4.io/ntention/f_auto/Researchposts/b68bf460-8aea-4891-9d89-0eaf47afe759.Jpeg'
postType: /research
categories:
    - category: Research
    - category: Interaction
excerpt: >-
  Combining ‘Mixed Input’ with ‘Contextual Information’, whether it is in AR, VR, MR or when controlling machines, the smart interaction system is able to understand the intent of the user and pass on the correct command.
meta:
  description: Combining ‘Mixed Input’ with ‘Contextual Information’, whether it is in AR, VR, MR or when controlling machines, the smart interaction system is able to understand the intent of the user and pass on the correct command.
  title: Combining Context and Input Methods
---
####Imagine combining varous input methods
Our vision for the interaction system of the future is centered around ‘Mixed Input’. Using a wide array of input methods, the user can combine different ways of interacting with both software and hardware for efficient communication and an optimal workflow. Imagine combining a hand interface, voice commands, eye tracking, body language, even brain waves and neural impulses, to interact with a machine.

The flexible interaction system requires a mix of input methods and an understanding of the context under which the input event takes place, to efficiently capture the intent of the user. Combining ‘Mixed Input’ with ‘Contextual Information’, whether it is in AR, VR, MR or when controlling machines, the smart interaction system is able to understand the intent of the user and pass on the correct command.
